# chat-pdf-local-llm
a simple webapp to chat directly with an local llm, using google/flan-t5-large as the backbone model.
the objective of the project is not to get the absolute best performance, but to show what's possible with the tech stack.
a deeper llm is neccesary. also in the future i'd like to try to combine a language model with [TAPAS](https://github.com/google-research/tapas)

python version: Python 3.10.12
